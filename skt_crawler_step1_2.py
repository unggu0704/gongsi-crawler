import requests
import json
import time
import re
import random
from datetime import datetime
import pandas as pd
from concurrent.futures import ThreadPoolExecutor, as_completed
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
import urllib3

# SSL ê²½ê³  ë¬´ì‹œ
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

class SKTStableCrawler:
    def __init__(self):
        self.base_url = "https://shop.tworld.co.kr"
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36',
            'Accept': 'application/json, text/javascript, */*; q=0.01',
            'Referer': 'https://shop.tworld.co.kr/wireline/plan/list'
        }
        self.scrb_type_map = {'31': 'ê¸°ê¸°ë³€ê²½', '32': 'ë²ˆí˜¸ì´ë™', '33': 'ì‹ ê·œê°€ì…'}
        
        # ì„¸ì…˜ ë° ì¬ì‹œë„ ì „ëµ (ì•ˆì •ì„± ê·¹ëŒ€í™”)
        self.session = requests.Session()
        retry_strategy = Retry(
            total=5,  # ì¬ì‹œë„ íšŸìˆ˜ ìƒí–¥
            backoff_factor=1.5, # ì§€ìˆ˜ ë°±ì˜¤í”„ ì ìš©
            status_forcelist=[429, 500, 502, 503, 504]
        )
        adapter = HTTPAdapter(max_retries=retry_strategy, pool_connections=10, pool_maxsize=20)
        self.session.mount("https://", adapter)

    def get_categories(self):
        url = f"{self.base_url}/api/wireless/subscription/category"
        try:
            resp = self.session.get(url, params={'categoryId': '20010001'}, headers=self.headers, verify=False, timeout=10)
            resp.raise_for_status()
            return resp.json().get('content', [])
        except Exception as e:
            print(f"âŒ ì¹´í…Œê³ ë¦¬ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return []

    def get_subscriptions(self, cat_id):
        url = f"{self.base_url}/api/wireless/subscription/list"
        params = {'type': 1, 'upCategoryId': '300100400001', 'categoryId': cat_id, '_': int(time.time() * 1000)}
        try:
            resp = self.session.get(url, params=params, headers=self.headers, verify=False, timeout=10)
            return resp.json().get('content', [])
        except Exception as e:
            return []

    def fetch_subsidy_worker(self, task):
        """ë‹¨ì¼ ìš”ì²­ ì²˜ë¦¬ ë° ì •êµí•œ ì˜ˆì™¸ ì²˜ë¦¬"""
        sub_id = task['id']
        sub_nm = task['nm']
        s_type = task['type']
        month = task['month']
        
        # ì„œë²„ íƒì§€ ë°©ì§€ë¥¼ ìœ„í•œ ë¯¸ì„¸ ëœë¤ ì§€ì—°
        time.sleep(random.uniform(0.05, 0.15))
        
        url = f"{self.base_url}/notice"
        params = {'prodId': sub_id, 'scrbType': s_type, 'saleMonth': month}
        
        try:
            resp = self.session.get(url, params=params, headers=self.headers, verify=False, timeout=15)
            if resp.status_code != 200:
                return []

            # ì •ê·œì‹ íŒ¨í„´ ì•ˆì •í™” (ë©€í‹°ë¼ì¸ ëŒ€ì‘)
            match = re.search(r'parseObject\(\s*(\[.*?\])\s*\);', resp.text, re.DOTALL)
            if not match:
                return []

            raw_data = json.loads(match.group(1))
            extracted = []
            for item in raw_data:
                extracted.append({
                    'ì œì¡°ì‚¬': item.get('companyNm'),
                    'ë‹¨ë§ëª…': item.get('productNm'),
                    'ìš©ëŸ‰': item.get('productMem'),
                    'ìš”ê¸ˆì œëª…': sub_nm,
                    'ê°€ì…ìœ í˜•': self.scrb_type_map.get(s_type),
                    'ì•½ì •ê¸°ê°„': f"{month}ê°œì›”",
                    'ì¶œê³ ê°€': item.get('factoryPrice', 0),
                    'ê³µì‹œì§€ì›ê¸ˆ': item.get('telecomSaleAmt', 0),
                    'ì¶”ê°€ì§€ì›ê¸ˆ': item.get('selDsnetSupmAmt', 0),
                    'ì‹¤êµ¬ë§¤ê°€': item.get('price', 0),
                    'ê³µì‹œì¼': item.get('effStaDt')
                })
            return extracted
        except Exception:
            return []

    def run(self, max_threads=5):
        print("ğŸ” 1, 2ë‹¨ê³„: ìš”ê¸ˆì œ ëª©ë¡ êµ¬ì„± ì¤‘...")
        categories = self.get_categories()
        all_tasks = []
        
        for cat in categories:
            subs = self.get_subscriptions(cat['categoryId'])
            for s in subs:
                for t in ['31', '32', '33']:
                    for m in ['12', '24']:
                        all_tasks.append({'id': s['subscriptionId'], 'nm': s['subscriptionNm'], 'type': t, 'month': m})

        total_tasks = len(all_tasks)
        print(f"âœ… ì´ {total_tasks}ê°œì˜ ì¡°íšŒ ì¡°í•© ìƒì„±ë¨. ë³‘ë ¬ ìˆ˜ì§‘ ì‹œì‘ (Thread: {max_threads})")

        final_data = []
        with ThreadPoolExecutor(max_workers=max_threads) as executor:
            futures = {executor.submit(self.fetch_subsidy_worker, task): task for task in all_tasks}
            
            for i, future in enumerate(as_completed(futures), 1):
                res = future.result()
                if res:
                    final_data.extend(res)
                
                if i % 100 == 0 or i == total_tasks:
                    print(f"ğŸ“Š ì§„í–‰ë¥ : {i}/{total_tasks} ({i/total_tasks*100:.1f}%) ì™„ë£Œ")

        if final_data:
            df = pd.DataFrame(final_data)
            fname = f"skt_subsidy_final_{datetime.now().strftime('%H%M%S')}.xlsx"
            df.to_excel(fname, index=False)
            print(f"\nğŸ‰ ìˆ˜ì§‘ ì„±ê³µ! íŒŒì¼ëª…: {fname} (ë°ì´í„°: {len(final_data)}ê±´)")
        else:
            print("\nâŒ ìˆ˜ì§‘ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ì‚¬ì´íŠ¸ êµ¬ì¡°ë¥¼ í™•ì¸í•˜ì„¸ìš”.")

if __name__ == "__main__":
    crawler = SKTStableCrawler()
    # ì•ˆì •ì„±ì„ ìœ„í•´ 5ê°œ ìŠ¤ë ˆë“œ ê¶Œì¥
    crawler.run(max_threads=5)